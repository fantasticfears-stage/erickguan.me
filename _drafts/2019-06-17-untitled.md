---
layout: post
title: The way Apache Beam talks - details of implementation
categories: 
  - consectetur
  - malesuada
tags:
  - semper
  - fermentum
---

Recent projects like Tensorflow, PyTorch, and Apache Beam have interesting design and implementations. Apache Beam provides a set of SDKs for defining and executing data pipeline as well as runners opearting on different platforms. A new sort of “language” regardless of the SDKs is defined.

[Apache Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/) introduces the most concepts defined from the dataflow paper (mentioned in the guide). `PCollection` represents the same type of elements and `PTransform` defines the way to mutate data in a `PCollection`.

As the development documentation says, this is a large scale project. Implementing SDKs and runners will take significant amount of work. But the existing code is abundant for comparing and referring.

# How a pipeline works

So the pipeline needs to be defined by a program provided by SDKs. Then those actions will be validated and then executed by the runner. The intermediate state stored in a DAG graph. Data platform grows huge indicates how complex data and machines we use for processing can be.

# Pipeline

Pipeline is the core object for users. Users need to define the data pipeline. And all those transformation formulates a DAG which can be executed by runners. SDKs and the DAG are implemented by using the same programing language. Unfortunately, language agnoistics is not the goal for this project. But looking into how Google implements its cloud runner, it’s possible to build such layer. The [Portability API](https://beam.apache.org/roadmap/portability/) is the development document for this issue. With such abstraction, the debug experience is going to be difficult. Luckily, it is terrible with current implementation.

`org.apache.beam.sdk.Pipeline` has many methods for defining input data and data transformation. Those actions are managed and stored as a DAG in this class.

## Add new transform

The core function would be `ApplyInternal`:

```java
private <InputT extends PInput, OutputT extends POutput> OutputT applyInternal(
  String name, InputT input, PTransform<? super InputT, OutputT> transform) {
  // ...
  transforms.pushNode(uniqueName, input, transform);
  try {
    transforms.finishSpecifyingInput();
    OutputT output = transform.expand(input);
    transforms.setOutput(output);
    return output;
  } finally {
    transforms.popNode();
  }
}
```

`transforms` is a `TransformHierarchy` class keeping DAG. We can see a few properties:

```java
private final Node root;
private final Map<Node, PInput> unexpandedInputs; private final Map<POutput, Node> producers;
// A map of PValue to the PInput the producing PTransform is applied to
private final Map<PValue, PInput> producerInput;
// Maintain a stack based on the enclosing nodes private Node current;
```
